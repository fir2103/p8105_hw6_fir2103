p8105_hw6_fir2103
================
Farizah Rob
2022-12-01

### Due date

Due: December 3 at 11:59pm.

### Points

| Problem   | Points |
|:----------|:-------|
| Problem 0 | 20     |
| Problem 1 | –      |
| Problem 2 | 40     |
| Problem 3 | 40     |

### Problem 0

This “problem” focuses on structure of your assignment, including the
use of R Markdown to write reproducible reports, the use of R Projects
to organize your work, the use of relative paths to load data, and the
naming structure for your files.

To that end:

-   create a public GitHub repo + local R Project for this assignment
-   write solutions using a .Rmd file that outputs a `github_document` /
    .md file
-   submit a link to your repo via Courseworks

Your solutions to Problems 1 and 2 should be implemented in your .Rmd
file, and your git commit history should reflect the process you used to
solve these Problems.

For Problem 0, we will assess adherence to the instructions above
regarding repo structure, git commit history, and whether we are able to
knit your .Rmd to ensure that your work is reproducible. Adherence to
appropriate styling and clarity of code will be assessed in Problems 1+
using the homework [style rubric](homework_style_rubric.html).

This homework includes figures; the readability of your embedded plots
(e.g. font sizes, axis labels, titles) will be assessed in Problems 1+.

### Problem 1

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  dplyr::select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2022-10-05 19:32:51 (8.408)

    ## file min/max dates: 1869-01-01 / 2022-10-31

``` r
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  dplyr::select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  dplyr::select(-strap, -models) %>% 
  unnest(results) %>% 
  dplyr::select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

### Problem 2

``` r
homicide_data <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
homicide_tidy <- 
  homicide_data %>%
  mutate(city_state = str_c(city, state, sep = ", "), 
         solved = ifelse(disposition == "Closed by arrest", 1, 0),
         victim_age = as.numeric(victim_age)) %>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black"),
         victim_sex != "Unknown")
```

Fit a logistic regression model for Baltimore, MD

``` r
baltimore_data <- homicide_tidy %>% 
  filter(city_state == "Baltimore, MD")

model_1 <- glm(solved ~ victim_age + victim_sex + victim_race, data = baltimore_data, family = binomial()) 

model_1_summ <- model_1 %>% 
  broom::tidy(conf.int = TRUE) %>%
  mutate(OR = exp(estimate), 
         OR_conf.low = exp(conf.low), 
         OR_conf.high = exp(conf.high)) %>%
  filter(term  == "victim_sexMale") %>%
  dplyr::select(term, OR, OR_conf.low, OR_conf.high, p.value) %>%
  knitr::kable(digits = 4)

model_1_summ
```

| term           |     OR | OR_conf.low | OR_conf.high | p.value |
|:---------------|-------:|------------:|-------------:|--------:|
| victim_sexMale | 0.4255 |      0.3242 |       0.5576 |       0 |

Since the odds ratio is less than 1 (0.43) and female gender is the
baseline category, solving homicides with female victims is less likely
than solving homicides with male victims (about a 57% decrease in
likelihood).

``` r
glm_all <- homicide_tidy %>%
  group_by(city_state) %>%
  nest() %>%
  mutate(models = map(data, ~glm(solved ~ victim_age + victim_sex + victim_race, data =., family = binomial())), 
         results = map(models, broom::tidy, conf.int = TRUE)) %>%
  dplyr::select(city_state, models, results) %>%
  unnest(cols = results)

glm_or <- glm_all %>%
  mutate(OR = exp(estimate), 
         OR_conf.low = exp(conf.low), 
         OR_conf.high = exp(conf.high)) %>%
  dplyr::select(city_state, term, OR, OR_conf.low, OR_conf.high, p.value) %>%
  filter(term == "victim_sexMale") 

glm_or
```

    ## # A tibble: 47 × 6
    ## # Groups:   city_state [47]
    ##    city_state      term              OR OR_conf.low OR_conf.high  p.value
    ##    <chr>           <chr>          <dbl>       <dbl>        <dbl>    <dbl>
    ##  1 Albuquerque, NM victim_sexMale 1.77        0.825        3.76  1.39e- 1
    ##  2 Atlanta, GA     victim_sexMale 1.00        0.680        1.46  1.00e+ 0
    ##  3 Baltimore, MD   victim_sexMale 0.426       0.324        0.558 6.26e-10
    ##  4 Baton Rouge, LA victim_sexMale 0.381       0.204        0.684 1.65e- 3
    ##  5 Birmingham, AL  victim_sexMale 0.870       0.571        1.31  5.11e- 1
    ##  6 Boston, MA      victim_sexMale 0.667       0.351        1.26  2.12e- 1
    ##  7 Buffalo, NY     victim_sexMale 0.521       0.288        0.936 2.90e- 2
    ##  8 Charlotte, NC   victim_sexMale 0.884       0.551        1.39  6.00e- 1
    ##  9 Chicago, IL     victim_sexMale 0.410       0.336        0.501 1.86e-18
    ## 10 Cincinnati, OH  victim_sexMale 0.400       0.231        0.667 6.49e- 4
    ## # … with 37 more rows

``` r
#plot
glm_or %>%
  ggplot(aes(x = fct_reorder(city_state, OR), y = OR)) + geom_point() + 
  geom_errorbar(aes(ymin = OR_conf.low, ymax = OR_conf.high)) + 
  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +
  labs(x = "City, State", title = "95% CI's of adjusted odds ratios for solving homicides comparing male victims to female victims")
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

For most cities, the odds of solving homicides with female victims is
less likely (lower) than the odds of solving homicides with male
victims. For the cities of Stockton, CA and Albuquerque, NM - the odds
of solving homicides with female victims is higher than the odds of
solving homicides with male victims. For those cities with odds ratio \>
1, the confidence intervals tend to be wider.

### Problem 3

``` r
birthwt_df <- read_csv("birthweight.csv")
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#convert babysex to binary, Male = 1, Female = 0

birthwt_tidy <- birthwt_df %>%
  janitor::clean_names() %>%
  mutate(babysex = ifelse(babysex == 1, 1, 0), 
         frace = factor(frace), 
         mrace = factor(mrace))

#check for missing data 

birthwt_tidy[!complete.cases(birthwt_tidy),]
```

    ## # A tibble: 0 × 20
    ## # … with 20 variables: babysex <dbl>, bhead <dbl>, blength <dbl>, bwt <dbl>,
    ## #   delwt <dbl>, fincome <dbl>, frace <fct>, gaweeks <dbl>, malform <dbl>,
    ## #   menarche <dbl>, mheight <dbl>, momage <dbl>, mrace <fct>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>

For data cleaning, the `babysex` variable was converted to a binary
variable with levels 1 and 0 (1 = male, 0 = female). The father’s race
and mother’s race variables were converted to factor variables. The
birthweight dataset has 4342 rows and 20 columns. There are no missing
values in the dataset.

Since `bwt` (birthweight) is a continuous, numeric variable - we will
build a linear regression model to predict it. Before fitting a model, I
conducted EDA and created some pairwise scatterplots (below) of
birthweight against other variables in the dataset to see if there is
any obvious association.

``` r
#look at continuous variables related to baby

pairs(bwt ~ bhead + blength + gaweeks, data = birthwt_tidy)
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

``` r
# look at continuous variables related to mother

pairs(bwt ~ delwt + menarche + mheight + momage, data = birthwt_tidy)
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-7-2.png" width="90%" />

``` r
cor(birthwt_df$bwt, y = birthwt_df[, -4], use = "everything") %>%
  as_tibble()
```

    ## # A tibble: 1 × 19
    ##   babysex bhead blength delwt fincome  frace gaweeks malform menarche mheight
    ##     <dbl> <dbl>   <dbl> <dbl>   <dbl>  <dbl>   <dbl>   <dbl>    <dbl>   <dbl>
    ## 1 -0.0866 0.747   0.743 0.288   0.155 -0.179   0.412 0.00133  -0.0244   0.192
    ## # … with 9 more variables: momage <dbl>, mrace <dbl>, parity <dbl>,
    ## #   pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>, ppwt <dbl>, smoken <dbl>,
    ## #   wtgain <dbl>

``` r
#high correlation with bhead, blength in terms of correlation coefficient 
#other factors that could be important are delwt, momage, smoken, gaweeks
```

From EDA, some variables that could be important predictors are bhead,
blength, delwt, gaweeks, momage and smoken. I carried out stepwise
backward selection by AIC using the `stepAIC` function from the package
`MASS`. We started with the full model and removed predictors.

``` r
full_model <- lm(bwt ~., data = birthwt_tidy)
step_model <- stepAIC(full_model, direction = "backward", 
                      trace = FALSE)
summary(step_model)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    ##     gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthwt_tidy)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1097.18  -185.52    -3.39   174.14  2353.44 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -6070.2639   136.9081 -44.338  < 2e-16 ***
    ## babysex       -28.5580     8.4549  -3.378 0.000737 ***
    ## bhead         130.7770     3.4466  37.944  < 2e-16 ***
    ## blength        74.9471     2.0190  37.120  < 2e-16 ***
    ## delwt           4.1067     0.3921  10.475  < 2e-16 ***
    ## fincome         0.3180     0.1747   1.820 0.068844 .  
    ## gaweeks        11.5925     1.4621   7.929 2.79e-15 ***
    ## mheight         6.5940     1.7849   3.694 0.000223 ***
    ## mrace2       -138.7925     9.9071 -14.009  < 2e-16 ***
    ## mrace3        -74.8868    42.3146  -1.770 0.076837 .  
    ## mrace4       -100.6781    19.3247  -5.210 1.98e-07 ***
    ## parity         96.3047    40.3362   2.388 0.017004 *  
    ## ppwt           -2.6756     0.4274  -6.261 4.20e-10 ***
    ## smoken         -4.8434     0.5856  -8.271  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 272.3 on 4328 degrees of freedom
    ## Multiple R-squared:  0.7181, Adjusted R-squared:  0.7173 
    ## F-statistic: 848.1 on 13 and 4328 DF,  p-value: < 2.2e-16

Stepwise backward selection consists of the predictors: `babysex`,
`blength`, `bhead`, `delwwt`, `fincome`, `gaweeks`, `mheight`, `mrace`,
`parity`, `ppwt`, and `smoken`. Therefore, I am fitting a linear
regression model with these predictors. The adjusted R-squared is 0.7173
which is quite high and suggests a decent model fit.

``` r
my_mod <- lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthwt_tidy)
summary(my_mod)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    ##     gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthwt_tidy)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1097.18  -185.52    -3.39   174.14  2353.44 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -6070.2639   136.9081 -44.338  < 2e-16 ***
    ## babysex       -28.5580     8.4549  -3.378 0.000737 ***
    ## bhead         130.7770     3.4466  37.944  < 2e-16 ***
    ## blength        74.9471     2.0190  37.120  < 2e-16 ***
    ## delwt           4.1067     0.3921  10.475  < 2e-16 ***
    ## fincome         0.3180     0.1747   1.820 0.068844 .  
    ## gaweeks        11.5925     1.4621   7.929 2.79e-15 ***
    ## mheight         6.5940     1.7849   3.694 0.000223 ***
    ## mrace2       -138.7925     9.9071 -14.009  < 2e-16 ***
    ## mrace3        -74.8868    42.3146  -1.770 0.076837 .  
    ## mrace4       -100.6781    19.3247  -5.210 1.98e-07 ***
    ## parity         96.3047    40.3362   2.388 0.017004 *  
    ## ppwt           -2.6756     0.4274  -6.261 4.20e-10 ***
    ## smoken         -4.8434     0.5856  -8.271  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 272.3 on 4328 degrees of freedom
    ## Multiple R-squared:  0.7181, Adjusted R-squared:  0.7173 
    ## F-statistic: 848.1 on 13 and 4328 DF,  p-value: < 2.2e-16

Plot of residual vs fitted values

``` r
birthwt_tidy %>%
  modelr::add_predictions(my_mod) %>%
  modelr::add_residuals(my_mod) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Fitted Values", y = "Residuals", title = "Residual vs Fitted Plot")
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />
The residuals vs fitted values seems more or less random, and there is
no specific pattern which is the type of pattern we are looking for.

### Cross-validation and model comparison

``` r
cv_df <-
  crossv_mc(birthwt_tidy, 100)

train_df <- cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
test_df <- cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df <-
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

### Comparing models

``` r
second_mod <- lm(bwt ~ blength + gaweeks, data = birthwt_tidy)
summary(second_mod)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ blength + gaweeks, data = birthwt_tidy)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1709.6  -215.4   -11.4   208.2  4188.8 
    ## 
    ## Coefficients:
    ##              Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) -4347.667     97.958  -44.38   <2e-16 ***
    ## blength       128.556      1.990   64.60   <2e-16 ***
    ## gaweeks        27.047      1.718   15.74   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 333.2 on 4339 degrees of freedom
    ## Multiple R-squared:  0.5769, Adjusted R-squared:  0.5767 
    ## F-statistic:  2958 on 2 and 4339 DF,  p-value: < 2.2e-16

``` r
third_mod <- lm(bwt ~ bhead*blength*babysex, data = birthwt_tidy)
summary(third_mod)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ bhead * blength * babysex, data = birthwt_tidy)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1132.99  -190.42   -10.33   178.63  2617.96 
    ## 
    ## Coefficients:
    ##                         Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)            -801.9487  1102.3077  -0.728 0.466948    
    ## bhead                   -16.5975    34.0916  -0.487 0.626388    
    ## blength                 -21.6460    23.3720  -0.926 0.354421    
    ## babysex               -6374.8684  1677.7669  -3.800 0.000147 ***
    ## bhead:blength             3.3244     0.7126   4.666 3.17e-06 ***
    ## bhead:babysex           198.3932    51.0917   3.883 0.000105 ***
    ## blength:babysex         123.7729    35.1185   3.524 0.000429 ***
    ## bhead:blength:babysex    -3.8781     1.0566  -3.670 0.000245 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 287.7 on 4334 degrees of freedom
    ## Multiple R-squared:  0.6849, Adjusted R-squared:  0.6844 
    ## F-statistic:  1346 on 7 and 4334 DF,  p-value: < 2.2e-16

The adjusted r-squared of the model with `blength` and `gaweeks` as main
effects is 0.5767, the adjusted r-squared of the model with the
interaction terms of `bhead`, `blength`, and `babysex` is 0.6844. Both
R-squared values are lower than that of my model, therefore suggesting a
poorer fit.

``` r
cv_df <-
  cv_df %>% 
  mutate(
    my_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    second_mod  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    third_mod  = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_second = map2_dbl(second_mod, test, ~rmse(model = .x, data = .y)),
    rmse_third = map2_dbl(third_mod, test, ~rmse(model = .x, data = .y)))


cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

<img src="p8105_hw6_fir2103_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

Plotting the prediction error in the above violin plot for all three
models, we can see that the best predictive accuracy is achieved by my
model. The model with main and interaction effects of `bhead`,
`blength`, and `babysex` is also close to my model, whereas the model
with only the main effects of `blength` and `gaweeks` is the worst in
its predictive accuracy.
